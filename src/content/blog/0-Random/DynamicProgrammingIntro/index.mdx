---
published: true
title: Dynamic Programming is Ez
slug: dynamic-programming
date: 2024-10-02
cover: images/coin_change_5.png
tags: ["Algorithms", "Dynamic Programming", "Optimization"]
category: "Algorithms"
excerpt: "Master dynamic programming with step-by-step examples, starting from Fibonacci."
pinned: true
---

Dynamic programming is a common nemesis in coding interviews. People see "optimal substructure" and "overlapping subproblems" and immediately panic. I realized DP isn't the dark arts, it's just a systematic way to avoid doing the same work twice.

Once you understand the reasoning patterns, DP problems become much more approachable. This post breaks down the step-by-step thinking that turns intimidating problems into manageable solutions. Whether you're prepping for interviews or just want to understand how these algorithms work, we'll start with Fibonacci and build up to the classic problems you'll encounter on LeetCode and in technical interviews.

# Recognize When You Need It

Before diving into fancy terminology, here's when you should reach for DP:

- You're solving the same subproblems over and over (`fib(3)` multiple times)
- You're making choices that affect future choices (items in knapsack)
- The brute force solution is exponential but *feels* like it shouldn't be

The technical terms matter too:
1. **Overlapping subproblems**: You keep solving the same smaller problems
2. **Optimal substructure**: The best solution uses best solutions to smaller problems

But here's the practical test: if your recursive solution is painfully slow and you're recalculating things, DP can probably help mitigate the repetition.

# Why Fibonacci Breaks Your Computer

Fibonacci seems innocent enough, each number is just the sum of the two before it. The recursive solution looks clean and mirrors the mathematical definition perfectly:

```python
def fib(n):
    if n < 2:
        return n
    return fib(n-1) + fib(n-2)
```

This works great for small numbers. `fib(5)` runs instantly. `fib(10)` is fine. But try `fib(40)` and go make coffee because you'll be waiting a while.

We need to consider why the problem scales exponentially when intuitively it shouldn't? If I asked you to calculate `fib(5)` by hand, you'd probably do a sliding window of size 2 starting with 0 and 1 and going from there on paper.

```
fib(5) = fib(4) + fib(3)
fib(4) = fib(3) + fib(2)
fib(3) = fib(2) + fib(1)
fib(2) = fib(1) + fib(0)
fib(1) = 1
fib(0) = 0
```

Which feels entirely linear no? But it's not. The recursive solution is actually exponential because it recalculates the same subproblems over and over again.

To better understand the execution order, here's how the recursive calls unfold:

![Fibonacci Call Order](images/fib_5_anim.gif)

We see that the tree is height `n` and for each node, we calculate 2 other function calls. This in turn means we will have `2^n` total function calls, exponentially growing with `n`.

TODO: Some way of showing how this expands like crazy

Follow the call order again, consider how many times we hit a call that we've already solved before. What if we could just save the results of these calls and return them and avoid traversing down paths we've already seen before?

![Fibonacci Memoization](images/fib_5.png)

Notice the blue nodes? Those represent subproblems we've already solved! For `fib(5)`, we calculate `fib(3)` twice and `fib(2)` three times. This redundancy grows exponentially as n increases, consider what this looks like for `fib(8)`.

![Fibonacci Memoization](images/fib_8.png)

All the grey nodes are calculations we get to avoid because we saved the results of prior problems and time we save! In practice, we linearized an exponential solution. I'll detail more about how to implement this, but for now just know it's called **memoization**.

# The Performance Disaster

The performance difference isn't just bad, it's catastrophically bad:

![Performance Comparison](images/fib_comp_log.png)

Here are some real numbers that'll make you appreciate memoization:
- `fib(25)`: 242,785 function calls vs 49 calls (5,000x improvement)
- `fib(35)`: ~30 seconds vs microseconds
- `fib(40)`: several minutes vs still microseconds

# Demystifying DP

This is the how I tackle DP problems for interviews, LeetCode, or [Project Euler](https://projecteuler.net/).

## Step 1: Think Recursively First

Don't worry about DP yet.

Just think: "How can I break this into smaller versions of the same problem?"

This step is crucial because DP problems are fundamentally recursive problems with optimizations. If you can't solve it recursively, you can't solve it with DP.
- What smaller subproblems do I need to solve?
- What are my base cases?
- What choices am I making at each step?

## Step 2: Write Out the Recurrence Relation

This is where the magic happens. For Fibonacci:
```
fib(n) = fib(n-1) + fib(n-2)
```

In interviews, I always write this out clearly. It shows you understand how the problem breaks down and gives you something concrete to implement.

## Step 3: The Base Cases

Base cases are where recursion stops. Missing these or getting them wrong will crash your solution:
```python
if n < 2:
    return n  # fib(0) = 0, fib(1) = 1
```

## Step 4: Naive Recursive Solution

Get the basic version working first. Don't optimize yet:

```python
def fib(n):
    if n < 2:
        return n
    return fib(n-1) + fib(n-2)
```

Test it on small inputs. If this doesn't work, neither will the DP version.

## Step 5: Recognize the Inefficiency

Run your naive solution on larger inputs. When it gets slow, you've found your overlapping subproblems. That's your cue that memoization will help.

## Step 6: Add Memoization

Now optimize by storing results you've already computed:

```python
def fib(n, memo={}):
    if n in memo:
        return memo[n]

    if n < 2:
        return n

    memo[n] = fib(n-1, memo) + fib(n-2, memo)
    return memo[n]
```

## The Critical Insight: State Definition

Here's what trips up most people: your function parameters must completely describe the current state of your problem.

For Fibonacci, just `n` tells us everything we need to know. But for more complex problems, you might need multiple parameters. The test is: given just the parameters, can you solve the problem from that point without any additional information?

This is crucial for interviews because it shows you understand the problem structure at a deep level.

# Key Principles for DP Success

1. **Complete State Encoding**: Your function parameters must fully describe the current state of the problem
2. **State Implies Position**: Given the state, you should know exactly where you are in the problem
3. **Reusable States**: When you describe your state, you can save and reuse the solution

# Beyond Fibonacci: More Examples

## Climbing Stairs

How many ways can you climb n stairs if you can take 1 or 2 steps at a time?

```python
def climb_stairs(n):
    if n < 0:
        return 0
    elif n == 0:  # Reached the top!
        return 1

    # Take steps
    return climb_stairs(n-1) + climb_stairs(n-2)
```

Notice the pattern? It's the same as Fibonacci! This illustrates how many DP problems share similar structures.

## Coin Change: A Classic Interview Problem

This is one of the most common DP problems you'll see. The key insight: for any amount, try every coin and take the best result.

![Coin Change Tree](images/coin_change_5.png)

**Reasoning process:**
1. **State**: What amount do I need to make? (That's our parameter)
2. **Choices**: I can use any coin that doesn't exceed the current amount
3. **Recurrence**: `min_coins(amount) = 1 + min(min_coins(amount - coin) for each coin)`

```python
def coin_change(coins, amount, memo={}):
    if amount < 0:
        return float('inf')  # Invalid - can't make negative amounts
    if amount == 0:
        return 0  # Base case - need 0 coins for amount 0
    if amount in memo:
        return memo[amount]

    best_count = float('inf')
    for coin in coins:
        coin_count = coin_change(coins, amount - coin, memo) + 1
        best_count = min(best_count, coin_count)

    memo[amount] = best_count
    return memo[amount]
```

The pattern is always the same: try all choices, take the best result.

## Minimum Path Sum

Find the minimum sum path from top-left to bottom-right in a grid (you can only move right or down):

![Minimum Path Sum Tree](images/min_path_sum.png)

```python
def min_path_sum(grid, memo={}):
    m, n = len(grid), len(grid[0])

    def step(r, c, memo):
        if (r, c) in memo:
            return memo[(r, c)]

        if r == m or c == n:
            return float('inf')

        if (r + 1, c + 1) == (m, n):  # Reached bottom-right
            return grid[r][c]

        grid_val = grid[r][c]
        memo[(r, c)] = min(step(r + 1, c, memo), step(r, c + 1, memo)) + grid_val
        return memo[(r, c)]

    return step(0, 0, memo)
```

This example shows how DP applies to 2D problems where you need to explore multiple paths to find the optimal solution.

## Longest Common Subsequence (LCS)

Find the length of the longest subsequence common to two strings. This is fundamental for diff algorithms, DNA analysis, and text comparison.

```python
def lcs_length(text1, text2):
    def dp(i, j, memo={}):
        # Base case: reached end of either string
        if i == len(text1) or j == len(text2):
            return 0

        if (i, j) in memo:
            return memo[(i, j)]

        # If characters match, include in LCS
        if text1[i] == text2[j]:
            memo[(i, j)] = 1 + dp(i + 1, j + 1, memo)
        else:
            # Try both: advance in text1 or text2
            memo[(i, j)] = max(dp(i + 1, j, memo), dp(i, j + 1, memo))

        return memo[(i, j)]

    return dp(0, 0)

# Example usage
text1 = "ABCDGH"
text2 = "AEDFHR"
print(f"LCS length: {lcs_length(text1, text2)}")  # Output: 3 (ADH)
```

This problem showcases the power of 2D state spaces in DP. The state `(i, j)` represents comparing `text1[i:]` with `text2[j:]`.

# Master the Pattern, Master DP

The beauty of DP is that once you understand the reasoning pattern, you can tackle any problem systematically. Here's the mental framework that works:

1. **Think recursively first** - break the problem down before optimizing
2. **Write the recurrence clearly** - this becomes your roadmap
3. **Get the base cases right** - these are your foundation
4. **Code the naive version** - prove your logic works
5. **Add memoization** - eliminate redundant work
6. **Define state carefully** - your parameters must capture everything needed

The biggest breakthrough for me was realizing that DP problems aren't about memorizing solutions - they're about recognizing patterns and applying a systematic thinking process.

Whether you're preparing for interviews or just want to understand these algorithms, practice this framework on different problems. Start with the classics, understand the reasoning, and you'll find that even complex DP problems become approachable puzzles rather than intimidating obstacles.

# Practice Problems to Try Next

- **House Robber**: Maximize money robbed without robbing adjacent houses
- **Edit Distance**: Minimum operations to transform one string to another
- **Longest Increasing Subsequence**: Find longest strictly increasing subsequence
- **Palindrome Partitioning**: Minimum cuts to partition string into palindromes
- **Maximum Product Subarray**: Find contiguous subarray with maximum product

The next time you encounter a problem that seems to require exploring multiple paths or making optimal choices at each step, ask yourself: "Could this benefit from dynamic programming?" The answer might just transform an impossible problem into a trivial one.
