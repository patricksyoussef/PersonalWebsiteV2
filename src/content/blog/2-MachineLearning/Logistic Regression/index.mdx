---

published: true
title: What is Logistic Regression?
slug: logistic-regression-intro
date: 2020-12-23
cover: images/ExampleTrainingCrop.png
tags: ["Machine Learning", "Optimization"]
category: "Machine Learning"
excerpt: "How to implement a logistic regression model with a neural network mindset in Python."
pinned: false
---

This post will introduce you to the principles behind logistic regression as a binary classification method. Using NumPy we'll implement the model from scratch.

There are different ways to implement this particular algorithm but I will focus on an implementation with a neural network mindset.

# What is Logistic Regression?

Logistic regression is a predictor borrowed from statistics used for binary classification. In english this means that we can use the model to determine is an input example belongs to a group or not. As an example, if we knew certain features about the weather (temperature, humidity, wind, etc.) we could try to predict if it's going to rain or not. To do so, we need many _labeled_ examples (inputs + did it rain) of data from other days.

# Representation

Let's first understand the required data and math for this problem.

## Data Organization

We represent a particular example as a vector of features and we store all these examples as one large matrix $X$ where $X_i$ is a particular example (a single day if we follow our prior rain metaphor). The labeled aspect means that we know whether or not that day had rain, we'll call this the ground-truth and save all labels for our examples in a vector $y$ where we store a 0 for the days it didn't rain and a 1 when it does rain.

As an example, let's imagine that we track 2 different aspects to describe each day such as average temperature and humidity. For a year, we would have 365 examples of temperature and humidity stored in our vector $X$ and the rain history in our vector $y$.

$$
X = [365 \times 2] =
\begin{bmatrix}
72 & 0.42\\
55 & 0.92\\
\vdots & \vdots \\
53 & 0.95\\
71 & 0.43\\
\end{bmatrix}
$$

$$
y=[365 \times 1] =
\begin{bmatrix}
0\\
1\\
\vdots \\
1\\
0\\
\end{bmatrix}
$$

## Forward Propagation

The forward propagation step is where we take an example day $X_i$ and pass it through our model to yield a prediction $\hat{y_i}$. We need a signal of how *important* each of the input signals is to whether it rained and in which direction it pushes our guess.

The importance as I called it is more aptly called the weight, we store a weight value for each of our input values (humidity and temperature). A positive weight implies that an increase in the input signal corresponds to a higher likelihood of predicting true. Across all our possible inputs, it's easier to combine all the weights into one vector $W$.

We may need to shift our baseline as well, to do this we include an offset that in a way sets our *start* for where we start allowing our inputs to change off of, called the bias $b$.

Lastly, we need a way of "squashing" the signal down to yes and no. This is best done as a probability where 0 says definetely not, and a 1 is very confident. We want to take the loudest of voice, and quietest of whispers and clamp them to a range we actually know how to handle. This is done with the sigmoid function $\sigma$.

$$
\hat{y_i} = \sigma (W^T X_i + b)
$$

![Sigmoid Function](images/sigmoid.jpg)

Where $\hat{y_i}$ is the probability that the example is in the class we are trying to predict. For the rain example, this is the logistic regressions estimated probability that it will rain that day. The sigmoid function is well catered to this as it introduces a non-linearity that bounds the output between 0 and 1, perfect for a probability!

### Example Forward Propagation

Let's consider our example $X$ in the prior data organization section. I understand that at this point I have not discussed how to determine $W$ and $b$ but for now follow that we know it, we will discuss how to find the correct values in the next section. Let's use the following:

$$
W^T = [1 \times 2] =
\begin{bmatrix}
0.1 & 10
\end{bmatrix}
$$

$$
b = -13
$$

We can actually go ahead and bulk propagate all examples using matrix multiplication! Let's use $X$ and $y$ as the define days from before. What we're looking at here are 4 days worth of information with their respective humidities and temperatures.

$$
X = [4 \times 2] =
\begin{bmatrix}
72 & 0.42\\
55 & 0.92\\
53 & 0.95\\
71 & 0.43\\
\end{bmatrix}
$$

We can see in the corresponding $y$ that it rained on days 2 and 3.

$$
y=[4 \times 1] =
\begin{bmatrix}
0\\
1\\
1\\
0\\
\end{bmatrix}
$$

Following the equation for $\hat{y_i}$ we get the following:

$$
\hat{y_i} = [4 \times 1] = \sigma (W^T X^T + b)
$$

$$
\hat{y_i} = \sigma (
\begin{bmatrix}
0.1 & 10
\end{bmatrix}

\begin{bmatrix}
72 & 55 & 53 & 71\\
0.42 & 0.92 & 0.95 & 0.43\\
\end{bmatrix}
-13
)
$$

$$
\hat{y_i} =
\sigma(
    \begin{bmatrix}
    -1.6 & 1.7 & 1.8 &-1.6 \\
    \end{bmatrix}
)
$$

$$
=
\begin{bmatrix}
0.17 & 0.85 & 0.86 & 0.17 \\
\end{bmatrix}
$$

When we compare those probabilities to the known results we can see they match! Days that had rain (days 2 and 3) show high probabilities that it rained and the non rain days respectively show low probabilities. In practice you can get a prediction as a 1 or 0 by rounding the probability, the idea is you round towards the prediction that is more likely in the probability. In the case of logistic regression where there are only two possibilities (rain or didn't) you can estimate one from the other very easily.

$$
\text{P}_{\text{Clear}} = 1 - \text{P}_{\text{Rain}}
$$

### Bias Simplification

Personally, I prefer to move the bias term ($b$) into the matrix multiplication ($W^T X_i$).

This is done by adding a column of 1's to the end of $X$, adding a feature that is consistent across all examples. Then we increase the size of the weights by 1 (now $W',X'$ etc.) such that the last weight acts as the bias. Check the math below if you are still curious.

This helps to simplify the training procedure as you only need to train for $W'$!

Moving forward, I will refer to $W'$ as just $W$.

> $$
> W^T X + b  = w_1 x_1 + ... + w_n x_n + b
> $$
>
> $$
> = w_1 x_1 + ... + w_n x_n + w_{n+1} [1,1,...,1]
> $$
>
> $$
> = W'X'
> $$

### Finding the Correct $W$ and $b$

It can be difficult/impossible to determine the correct values for the weights and bias through trial and error. Even for small examples of only two input features it's challenging to get correct, let alone when we step into many more.

To correctly determine the values we use a process called training!

# Training

Training is the process where we take many labeled examples and use them to determine values for $w$ that will yield us the best _overall_ performance. To be precise, we will describe some formulations for what are called the cost and loss of the model.

## Loss Function

Let's start with the loss function. The loss is a measure of the error for a particular example's prediction. Following our prior notation the loss for one example is:

$$
\mathcal{L} = -y \log(\hat{y}) - (1-y) \log(1-\hat{y})
$$

This function evaluates the error differently depending on the true label $y$. Let's play this out and see why it makes sense.

### Case 1: It Rained

When the actual outcome is rain ($y=1$), the loss simplifies to:

$$
\mathcal{L} = -(1) \log(\hat{y}) - (1 - 1)\log(1 - \hat{y})
$$

$$
\mathcal{L} = -\log(\hat{y})
$$

An ideal prediction would have $\hat{y} = 1$, resulting in:

$$
-\log(1) = 0
$$

Predicting any probability lower than 1 (less certain of rain) increases the loss.

### Case 2: It Did Not Rain

When there's no rain ($y=0$), the loss becomes:

$$
\mathcal{L} = -(0)\log(\hat{y}) - (1 - 0)\log(1 - \hat{y})
$$

$$
\mathcal{L} = -\log(1 - \hat{y})
$$

Here, the ideal prediction is $\hat{y} = 0$, yielding:

$$
-\log(1 - 0) = -\log(1) = 0
$$

Predicting higher probabilities incorrectly indicating rain increases the loss significantly.

### Intuition

The logistic regression loss function quantifies prediction errors for both outcomes ($y=0$ or $y=1$), penalizing predictions increasingly as they diverge from the observed reality.

## Cost Function

The cost function is easy, it's just the average of the loss functions so:

$$
C = \frac{1}{N}\sum_{i=1}^{N} \mathcal{L}_i
$$

The cost gives us an idea of how we are doing *overall* against all of our data.

We want to find a way to reduce this to the lowest value possible. To do so, we use an optimization method called gradient descent.

## Gradient Descent

This method isn't the focus of the post, so if this feels bit heavy don't worry too much. I introduce this as the method of choice because neural networks also use it so it's good to get exposure to it here!

Gradient descent is fairly simple, given the [gradient](https://www.youtube.com/watch?v=tIpKfDc295M&ab_channel=KhanAcademy) of a function with respect to the variable to be optimized take a step in the negative direction of the gradient (positive for gradient ascent) and update your inputs with the step. Keep repeating this until you are happy with the convergence. In general, it looks something like this:

$$
\theta_{i+1} = \theta_i - \alpha \nabla f(\theta)
$$

Where $\alpha$ is a tuneable parameter called the learning rate, it dictates what fraction of the gradient we should step by. I will not go through the derivation ([great resource](https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf)), but for our problem we would like to use gradient descent where $f$ is the cost function and we take the gradient in terms of $W$. For our use case this is:

$$
W_{i+1} = W_i - \alpha \frac{1}{N} X^T(\sigma(W^T X^T)-y)^T
$$

Now we have everything to implement!

# Implementation

Let's go ahead and import our python modules first as follow:

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
```

I will be using Python/NumPy for the implementation. The dataset used is imported in our prior code block with the rest of our libraries. The dataset takes a few human health metrics as features and tries to predict if the patient has breast cancer, you can read more [here](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html).

## Model Class

As we have many parameters, helper functions, and repeated operations I will be creating the model as a class. Let's initialize the class with all the methods we will be using.

```python
class LogisticRegression():

    def sigmoid(self, z):

    def prep_data(self, X):

    def add_dimension(self, X):

    def train(self, X_train, y_train, learning_rate, iterations):

    def predict(self, X_test):
```

- `sigmoid` will be where we write the sigmoid function
- `prep_data` is where we will normalize our data to between 0 and 1
- `add_dimension` is a helper function for adding the column of 1's to $X$
- `train` is where we will run the gradient descent iterations to determine our weights
- `predict` is where we will forward propagate on data to estimate the classes

Let's start with the `sigmoid` function. As a reminder the function is:

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

```python
def sigmoid(self, z):
    return 1 / (1 + np.exp(-z))
```

Let's continue with `prep_data` and `add_dimension`.

`prep_data` takes the range of each column and brings them down to $[0,1]$, that way no particular feature dominates the result of $W^T X_i$.

`add_dimension` creates a vector of ones the size of the number of samples and tacks it as the last column of $X$.

```python
def prep_data(self, X):
    temp = X - np.min(X, axis=0)
    X = temp / np.max(temp, axis=0)
    return X

def add_dimension(self, X):
    # Stacks a vector of ones with the same number of rows as X
    # horizontally on then end of X
    X_new = np.hstack((X, np.ones((X.shape[0], 1))))
    return X_new
```

The training process is going to be more involved, the comments explain what is going on but feel free to reach out if you would like more explanation.

```python
def train(self, X_train, y_train, learning_rate, iterations):
    self.learning_rate = learning_rate
    self.iterations = iterations

    # Prepare Data for Training
    X_train = self.prep_data(X_train)
    X_train = self.add_dimension(X_train)

    # Initialize Weights Randomly
    N,D = X_train.shape
    self.weights = np.random.random((D,1)) * 0.001

    # Iterate to train
    for i in range(self.iterations):

        # Compute the gradient
        A = self.sigmoid(np.dot(self.weights.T, X_train.T))
        grad = (1/N) * np.dot(X_train.T, (A - y_train).T)

        # Update Weights
        self.weights = self.weights - self.learning_rate * grad

        # Get Training Accuracy Every 100 Iterations
        if i % 100 == 0:
            y_est = self.predict(X_train)
            acc = np.sum(y_train == y_est) / y_train.size
            print("Training Accuracy on Iteration {}: {:0.4f}".format(i, acc))
```

To close out the model class, let's implement `predict` that computes the forward propagation of our test data.

```python
def predict(self, X_test):
    A = self.sigmoid(np.dot(self.weights.T, X_test.T))
    y_est = np.round(A)
    return y_est
```

## Data Import and Function Calls

Assuming you have downloaded the aforementioned dataset, you should make sure you have it in the same folder as your Python file! The following will import and split the data into a test and train set. I have not mentioned this yet but this is a good place to talk about it.

### Test/Train Split

Quick digression, a test and train split is a process of splitting your data into two groups based on a fraction (typical is 70/30 for train/test). This is very important to do as testing a model on the data it was trained on is not a good metric of its performance.

To put it briefly, a model can learn to fit a training set very well but won't generalize well to new data. This is very bad as all we really care about is how it generalizes. This is as much as you need to know for now, but I would highly recommend you read more [here](https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data).

```python
# Load in data
data = load_breast_cancer(return_X_y=True)

# Split into X and y
X = data[0]
y = data[1]

# Test Train Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30,
                                                    random_state=1)
```

## Running the Model

It's time to train our model and see how it does on the testing set!

```python output=6
# Create an instance of our logistic regression class
MyModel = LogisticRegression()

# Train the model on the training set using a learning rate of 0.1
# and 300 iterations
MyModel.train(X_train, y_train, 0.1, 300)

# Prep test data and get predictions for y_test
X_test = MyModel.prep_data(X_test)
X_test = MyModel.add_dimension(X_test)
y_test_est = MyModel.predict(X_test)

Training Accuracy on Iteration 50: 0.8894
Training Accuracy on Iteration 100: 0.9196
Training Accuracy on Iteration 150: 0.9221
Training Accuracy on Iteration 200: 0.9271
Training Accuracy on Iteration 250: 0.9271
Training Accuracy on Iteration 300: 0.9271
```

Now all we have to is compute the accuracy and see how we did.

```python output=1
# (Count of correct predictions) / (Total number of test samples)
test_accuracy = np.sum(y_test == y_test_est) / y_test.size
print("Test Accuracy: {:0.3f}".format(test_accuracy))

Test Accuracy: 0.912
```

This results in an accuracy of around 91%!

Considering the low number of features in the dataset this is a great result.

Hope you learned something new!
