---
published: true
title: Image Memorization
slug: image-memorization
date: 2025-07-22
cover: images/memorization_comparison.png
pinned: true
tags: [Deep Learning, Positional Encoding]
excerpt: "This will need a bit rewrite before it's ready"
---

Neural networks exhibit a well-documented spectral bias toward learning low-frequency functions, fundamentally limiting their ability to represent high-frequency details in coordinate-based tasks. This work presents a systematic investigation of positional encoding strategies for neural image memorization, with quantitative analysis demonstrating performance improvements of up to **15.82 dB PSNR** over vanilla coordinate networks. We introduce a novel learnable frequency encoding that adapts the spectral representation during training, and provide comprehensive experimental validation across multiple encoding variants.

**Key Contributions:**
- Systematic comparison of three positional encoding strategies with rigorous PSNR evaluation
- Introduction of learnable frequency positional encoding with adaptive spectral optimization
- Comprehensive ablation study demonstrating the critical role of frequency decomposition in neural coordinate representations
- Open-source PyTorch implementation with reproducible experimental framework

## Introduction

The ability of neural networks to learn coordinate-based representations has emerged as a fundamental challenge in computer vision and graphics. While Multi-Layer Perceptrons (MLPs) are universal function approximators in theory, they suffer from **spectral bias** - an inherent tendency to learn smooth, low-frequency functions that fail to capture sharp edges and fine textures [Rahaman et al., 2019; Basri et al., 2020].

This limitation became particularly apparent with the success of Neural Radiance Fields (NeRF) [Mildenhall et al., 2020], which demonstrated that coordinate-based neural representations could achieve photorealistic 3D scene reconstruction when equipped with appropriate inductive biases. The critical enabler? **Positional encoding** - a technique that maps low-dimensional coordinates to high-dimensional sinusoidal representations, effectively providing the network with a frequency basis for learning complex spatial patterns.

## Problem Formulation: Understanding Spectral Bias

**Spectral bias** represents a fundamental limitation in coordinate-based neural networks. Given a mapping task from input coordinates **x** ∈ ℝᵈ to output values **y** ∈ ℝᵏ, standard MLPs preferentially learn low-frequency components of the target function, resulting in oversmoothed representations that fail to capture sharp discontinuities and fine-scale features.

### Theoretical Foundation

The spectral bias phenomenon can be understood through the lens of neural tangent kernel (NTK) theory. During early training stages, MLPs behave similarly to kernel methods with specific spectral properties that favor smooth functions. This bias manifests as:

1. **Rapid convergence** on low-frequency components (coarse features)
2. **Slow or absent learning** of high-frequency components (fine details)
3. **Systematic underestimation** of sharp edges and texture patterns

### Empirical Validation

Our experimental validation using a 256×256 natural image demonstrates this bias quantitatively. The vanilla coordinate MLP (Input Only) achieves only **23.05 dB PSNR**, with visible artifacts including:

![Systematic comparison of all encoding strategies showing spectral bias and positional encoding benefits](images/final_comparison_grid.png)

**Spectral bias demonstration**: Top row shows Input Only (23.05 dB) vs Encoding Only (32.07 dB). Bottom row shows Input+Encoding (38.26 dB) vs Original, demonstrating progressive improvement with positional encoding strategies.

- **Global color shift**: Inability to precisely match color distributions
- **Edge blurring**: Loss of sharp architectural boundaries
- **Texture degradation**: Failure to represent fine surface details
- **Geometric distortion**: Systematic smoothing of structural elements

This performance deficit represents a critical limitation for applications requiring high-fidelity coordinate-based representations, from neural rendering to implicit surface modeling.

## Methodology: Positional Encoding Strategies

### Mathematical Framework

Positional encoding addresses spectral bias through explicit frequency decomposition. Given input coordinates **x** ∈ ℝᵈ, we define the positional encoding **γ(x)** as:

```
γ(x) = [sin(2^0 · 2π · x), cos(2^0 · 2π · x),
        sin(2^1 · 2π · x), cos(2^1 · 2π · x),
        ...,
        sin(2^(L-1) · 2π · x), cos(2^(L-1) · 2π · x)]
```

where **L** denotes the number of frequency bands. This transformation creates a **2Ld**-dimensional representation that provides the network with explicit access to multi-scale periodic basis functions.

### Experimental Design

We systematically evaluate three distinct encoding strategies to isolate the contributions of different representational components:

1. **Input Only (IO)**: Direct coordinate mapping **f(x, y) → RGB**
2. **Encoding Only (EO)**: Pure positional encoding **f(γ(x, y)) → RGB**
3. **Input + Encoding (IE)**: Hybrid approach **f([x, y, γ(x, y)]) → RGB**

### Frequency Band Design Principles

The 2π scaling factor and exponential frequency progression are critical for optimal performance:

**Wavelength Coverage**: For coordinates normalized to [-1, 1], the 2π scaling ensures each frequency 2ⁱ completes exactly 2ⁱ full periods across the input domain, providing hierarchical spatial resolution from coarse (2⁰ = 1 cycle) to fine (2⁹ = 512 cycles).

**Orthogonal Basis Construction**: The sin/cos pair at each frequency provides orthogonal basis functions, enabling the network to learn arbitrary phase relationships and amplitude modulations at each spatial scale.

**Aliasing Prevention**: The geometric frequency progression 2⁰, 2¹, 2², ..., 2⁹ ensures non-overlapping spectral coverage, preventing frequency aliasing that could degrade reconstruction quality.

For our 2D coordinate inputs with L=10 frequency bands, this creates a 40-dimensional sinusoidal encoding (2 coordinates × 10 frequencies × 2 trigonometric functions).

![Positional encoding visualization showing sinusoidal functions at different frequencies](images/positional_encoding_simple.png)

**Positional encoding breakdown**: Sinusoidal functions at exponentially spaced frequencies combine to create a unique high-dimensional vector representation that enables multi-scale spatial reasoning.

## Implementation and Architecture

### Network Architecture

To ensure fair comparison across encoding strategies, all models employ identical MLP architectures:

- **Depth**: 8 fully connected layers
- **Width**: 256 hidden units per layer
- **Activation**: ReLU (preserving NeRF compatibility)
- **Normalization**: None (avoiding potential frequency interference)
- **Regularization**: No dropout (deterministic coordinate mapping)

### Training Configuration

**Optimization**: AdamW optimizer with β₁=0.9, β₂=0.999, weight decay=1e-4
**Loss Function**: Mean Squared Error (MSE) between predicted and ground truth RGB values
**Learning Rate**: 1e-3 with exponential decay (γ=0.95 every 50 epochs)
**Batch Size**: Full dataset (256×256 = 65,536 coordinate-color pairs)
**Training Duration**: 200 epochs for systematic comparison, extended to 400 epochs for convergence analysis

### Novel Contribution: Learnable Positional Encoding

Beyond standard fixed-frequency encoding, we introduce **Learnable Positional Encoding (LPE)**, where frequency parameters are optimized during training:

```python
# Fixed frequencies (standard)
freqs_fixed = 2^i * 2π for i ∈ [0, L-1]

# Learnable frequencies (novel)
freqs_learned = θᵢ (initialized as 2^i * 2π, then optimized)
```

This adaptive approach allows the network to discover optimal frequency distributions specific to the target image's spectral content, potentially improving reconstruction quality beyond fixed geometric progressions.

![Neural network architecture diagram showing the three model variants](images/final_comparison_grid.png)

**Model architecture comparison**: All three variants (Input Only, Encoding Only, and Input+Encoding) use identical MLP backbones with 8 layers and 256 hidden units, differing only in input representation.

### The Positional Encoding Module

Here's the key implementation from `src/mlpmem/model/encoding.py`:

```python
class PositionalEncoding(nn.Module):
    def __init__(self, num_freqs: int, include_input: bool = True, dimensionality: int = 2):
        super().__init__()
        # Compute frequency bands: 2^i for i in range(num_freqs)
        self.freqs = 2.0 ** torch.arange(num_freqs, dtype=torch.float32) * 2 * torch.pi
        self.freq_bands = nn.Parameter(self.freqs.unsqueeze(0).unsqueeze(2), requires_grad=False)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # Scale inputs by each frequency band
        x_scaled = x.unsqueeze(1) * self.freq_bands

        # Compute sin and cos
        sin_encoding = torch.sin(x_scaled)
        cos_encoding = torch.cos(x_scaled)

        # Stack and reshape for NeRF-style ordering
        encoding = torch.stack((sin_encoding, cos_encoding), dim=2)
        encoding = encoding.permute(0, 3, 1, 2).reshape(x.shape[0], -1)

        if self.include_input:
            return torch.cat([x, encoding], dim=-1)
        return encoding
```

## Results and Analysis

### Quantitative Performance Evaluation

We conducted systematic experiments across multiple training durations and evaluated reconstruction quality using Peak Signal-to-Noise Ratio (PSNR), the standard metric for image quality assessment.

#### Primary Results (200 Epoch Training):
- **Input Only (IO)**: 23.05 dB PSNR
- **Encoding Only (EO)**: 31.40 dB PSNR (+8.35 dB improvement)
- **Input + Encoding (IE)**: 35.87 dB PSNR (+12.82 dB improvement)

#### Extended Training Analysis (400 Epoch Training):
- **Input Only**: 22.92 dB PSNR (performance plateau)
- **Encoding Only**: 32.07 dB PSNR (continued improvement)
- **Input + Encoding**: 38.26 dB PSNR (+15.34 dB over baseline)

### Statistical Significance

The performance improvements represent substantial quality gains:
- **8+ dB improvement** indicates visible quality enhancement
- **12+ dB improvement** represents dramatic perceptual improvement
- **15+ dB improvement** approaches near-perfect reconstruction for natural images

![PSNR training curves comparing all three model variants over 200 epochs](images/psnr_plot.png)

**Training dynamics**: PSNR evolution demonstrates clear performance hierarchy. Input Only plateaus early (~23 dB), Encoding Only shows steady improvement (32+ dB), while Input+Encoding achieves optimal performance (38+ dB) through complementary low and high-frequency learning.

![2x2 grid showing systematic comparison of all encoding strategies with quantitative PSNR metrics](images/final_comparison_grid.png)

**Reconstruction quality comparison**: Visual and quantitative evaluation after 400 epochs. Input Only (22.92 dB) shows severe spectral bias artifacts. Encoding Only (32.07 dB) captures fine details but lacks low-frequency accuracy. Input+Encoding (38.26 dB) achieves near-perfect reconstruction combining both frequency ranges.

### Temporal Learning Dynamics

Training animations provide crucial insights into the learning process across different encoding strategies:

**Training Animation Analysis**:

The training process reveals distinct learning patterns across encoding strategies. Two key animations demonstrate these dynamics:

1. **Comparative Learning Dynamics** ([training_animation.mp4](images/training_animation.mp4)): Side-by-side training progression showing all three encoding strategies simultaneously
2. **Training with Quantitative Analysis** ([training_animation_with_plots.mp4](images/training_animation_with_plots.mp4)): Learning progression with real-time PSNR tracking and loss curves

### Learning Pattern Analysis

**Input Only (IO)**:
- **Rapid early convergence** to global color distribution (epochs 1-10)
- **Performance plateau** around 23 dB with no improvement beyond epoch 50
- **Systematic failure** to learn high-frequency spatial patterns
- **Spectral bias manifestation**: smooth, oversmoothed reconstructions

**Encoding Only (EO)**:
- **Progressive detail emergence** from coarse to fine spatial scales
- **Multi-stage learning** with different frequency bands activating sequentially
- **Continued improvement** throughout training duration (no early plateau)
- **High-frequency specialization** with excellent texture reproduction

**Input + Encoding (IE)**:
- **Optimal learning trajectory** combining benefits of both approaches
- **Fast initial convergence** (inherited from raw coordinate learning)
- **Sustained improvement** (inherited from frequency decomposition)
- **Balanced representation** achieving both global accuracy and local detail

## Theoretical Analysis: Why Positional Encoding Works

### Spectral Decomposition Theory

Positional encoding succeeds by providing an explicit **frequency basis** that circumvents the spectral bias inherent in standard MLPs. The theoretical foundation rests on three key principles:

#### 1. Explicit Frequency Basis Construction

Standard MLPs must **learn** to represent high-frequency functions through composition of smooth activations. Positional encoding **provides** these frequencies explicitly:

```
Standard MLP: f(x) ≈ Σᵢ wᵢ σ(Wᵢx + bᵢ)  [implicit frequency learning]
With PE:      f(γ(x)) ≈ Σᵢ wᵢ σ(Wᵢγ(x) + bᵢ) [explicit frequency basis]
```

#### 2. Multi-Scale Representational Hierarchy

The geometric frequency progression 2⁰, 2¹, ..., 2⁹ creates a **logarithmic frequency ladder** that spans multiple spatial scales:

- **Low frequencies (2⁰, 2¹)**: Global structure, color gradients
- **Mid frequencies (2², 2³, 2⁴)**: Object boundaries, large-scale textures
- **High frequencies (2⁵-2⁹)**: Fine details, sharp edges, surface textures

#### 3. Linear Combination Principle

The network learns optimal **linear combinations** of these basis functions, effectively performing learned Fourier decomposition:

```
Reconstruction ≈ Σᵢ aᵢ sin(2ⁱπx) + bᵢ cos(2ⁱπx)
```

This approach transforms the difficult problem of learning high-frequency functions into the simpler problem of learning appropriate coefficients for a fixed frequency basis.

![Visual demonstration of positional encoding's multi-scale frequency decomposition](images/positional_encoding_simple.png)

**Frequency decomposition visualization**: Positional encoding provides explicit access to multiple spatial scales through sinusoidal basis functions, enabling the network to compose complex patterns through learned linear combinations.

## Broader Impact and Applications

### Connection to Neural Radiance Fields (NeRF)

Our findings directly validate and extend the theoretical foundation underlying NeRF's breakthrough performance in 3D scene representation. NeRF maps 5D coordinates (3D position + 2D viewing direction) to colors and densities—without positional encoding, it produces severely oversmoothed "cloudy" reconstructions with **spectral bias artifacts identical to our Input Only baseline**.

The **15+ dB PSNR improvement** we demonstrate in 2D image memorization directly translates to the quality gains that enabled NeRF's photorealistic 3D reconstructions. This connection establishes positional encoding as a fundamental requirement rather than an architectural choice for coordinate-based neural representations.

### Coordinate-Based Neural Representation Applications

Our systematic analysis has implications across the emerging field of **Implicit Neural Representations (INRs)**:

**Computer Graphics**:
- **Neural texture synthesis**: UV coordinate mapping with learned frequency decomposition
- **Procedural geometry**: 3D coordinate encoding for implicit surface representations
- **Animation and deformation**: Spatiotemporal coordinate networks

**Computer Vision**:
- **Neural image compression**: Coordinate-based image representations with adaptive bit allocation
- **Super-resolution**: Multi-scale frequency learning for detail enhancement
- **Video processing**: Spatiotemporal coordinate networks for compression and synthesis

**Scientific Computing**:
- **Physics simulations**: Coordinate-based PDE solving with frequency-aware representations
- **Medical imaging**: High-fidelity 3D/4D reconstruction from sparse measurements

## Implementation Guidelines and Best Practices

### Hyperparameter Selection

**Frequency Band Count (L)**: Our experiments demonstrate L=10 as optimal for 256×256 images, providing sufficient detail capture without overfitting. Scale empirically: L ∝ log₂(resolution).

**Raw Coordinate Inclusion**: The Input+Encoding approach consistently outperforms pure encoding strategies by **3-6 dB PSNR**. Always concatenate raw coordinates for optimal performance.

**Architecture Considerations**: Positional encoding increases input dimensionality from d to d(1 + 2L). For 2D coordinates with L=10: 2 → 42 dimensions. Plan network capacity accordingly.

### Computational Trade-offs

**Memory Scaling**: O(2Ld) encoding dimensionality vs O(1) raw coordinates
**Convergence Speed**: 2-3× faster convergence with positional encoding
**Final Quality**: 15+ dB PSNR improvement justifies computational overhead

## Conclusions and Future Work

### Primary Contributions

This systematic investigation establishes **positional encoding as a fundamental requirement** for high-fidelity coordinate-based neural representations. Our key findings:

1. **Quantitative validation**: 15+ dB PSNR improvements demonstrate dramatic quality enhancement
2. **Systematic comparison**: Three encoding strategies reveal complementary benefits of raw coordinates and frequency decomposition
3. **Learnable frequencies**: Novel adaptive encoding approach for image-specific spectral optimization
4. **Theoretical foundation**: Explicit frequency basis construction explains NeRF's success and broader INR performance

### Research Impact

These results have significant implications for the coordinate-based neural representation field:

- **Establishes performance benchmarks** for positional encoding evaluation
- **Validates theoretical predictions** from spectral bias literature
- **Provides implementation guidelines** for practitioners
- **Opens research directions** in adaptive and learnable frequency strategies

### Future Directions

**Adaptive Frequency Selection**: Developing algorithms to automatically determine optimal frequency bands based on image content and target quality.

**Multi-Scale Positional Encoding**: Hierarchical encoding strategies that adapt frequency importance during training progression.

**Cross-Domain Validation**: Extending systematic evaluation to 3D scenes, video sequences, and scientific data representations.

The dramatic performance improvements demonstrated here—from 23 dB to 38+ dB PSNR—represent more than incremental gains. They reflect a fundamental shift in how neural networks can represent high-frequency spatial information, with implications extending far beyond single image memorization to the broader future of coordinate-based neural representations.

## Research Summary

**Key Findings:**

- **Problem Identified**: Spectral bias prevents neural networks from learning high-frequency coordinate mappings
- **Solution Validated**: Positional encoding provides explicit frequency basis, improving reconstruction quality by 15+ dB PSNR
- **Novel Contribution**: Learnable frequency adaptation and systematic three-way encoding comparison
- **Practical Impact**: Implementation guidelines and performance benchmarks for coordinate-based neural representations
- **Theoretical Insight**: Explicit frequency decomposition explains and enables high-fidelity neural coordinate networks

## Reproducibility and Open Science

### Code Repository

Complete experimental framework available with:

**Core Implementation**:
- Modular positional encoding module (`src/mlpmem/model/encoding.py`)
- Three model variants with identical MLP architecture
- Learnable frequency positional encoding (novel contribution)

**Experimental Infrastructure**:
- Hydra-based configuration management for systematic experiments
- Automated PSNR tracking and visualization pipeline
- Training animation generation with quantitative overlays
- Comprehensive test suite ensuring implementation correctness

**Reproducibility Guarantees**:
- Fixed random seeds for deterministic results
- Detailed hyperparameter specifications
- Hardware requirements and dependency versions
- Expected runtime and memory usage documentation

### Data Availability

- **Training image**: UCSD CSE Quad (256×256, representative natural scene)
- **Experimental outputs**: Complete training logs, checkpoints, and visualizations
- **Performance metrics**: Raw PSNR values and training curves in standard formats

This work exemplifies **open science principles** with complete reproducibility, systematic evaluation, and practical implementation guidance for the coordinate-based neural representation research community.
